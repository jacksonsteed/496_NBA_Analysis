filtered <- na.omit(training_data$pts)
zzz <- na.omit(training_data$pts)
subset_undrafted <- drafted_0 %>% sample_n(8500, replace = FALSE)
mini_undrafted <- subset_undrafted %>% sample_frac(0.8)
small_train <- bind_rows(drafted_1, mini_undrafted)
small_test <- subset_undrafted %>% anti_join(small_train)
print(sum(is.na(small_train$pts)))
X_train <- small_train %>% select(`elevation (m)`, drtg, Ortg) %>% as.matrix()
y_train <- small_train$drafted
X_test <- small_test %>% select(`elevation (m)`, drtg, Ortg) %>% as.matrix()
y_test <- small_test$drafted
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = 'relu', input_shape = c(3)) %>%
layer_dense(units = 1, activation = 'sigmoid')
# Compile the model
model %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_adam(),   # use other optimizers like optimizer_sgd() if needed
metrics = c('accuracy')
)
# Train the model
history <- model %>% fit(
x = X_train,
y = y_train,
epochs = 20,             # Number of training iterations
batch_size = 32           # Number of samples per gradient update
)
accuracy <- model %>% evaluate(X, y)[[2]]
preprocess_data <- function(data) {
data[] <- lapply(data, function(x) {
if(is.numeric(x)) {
x[is.infinite(x) | is.nan(x)] <- 0
x[is.na(x)] <- mean(x, na.rm = TRUE)
return(x)
} else {
return(as.numeric(x))
}
})
return(data)
}
training_data <- preprocess_data(training_data)
testing_data <- preprocess_data(testing_data)
# Check for any remaining NAs
sum(is.na(small_train))
sum(is.na(small_test))
View(small_test)
small_train <- select(training_data, -pick, -player_name, -team)
small_test <- select(testing_data, -pick, -player_name, -team)
training_data <- preprocess_data(training_data)
testing_data <- preprocess_data(testing_data)
X_train <- small_train %>% select(`elevation (m)`, drtg, Ortg) %>% as.matrix()
y_train <- small_train$drafted
X_test <- small_test %>% select(`elevation (m)`, drtg, Ortg) %>% as.matrix()
y_test <- small_test$drafted
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = 'relu', input_shape = c(3)) %>%
layer_dense(units = 1, activation = 'sigmoid')
# Compile the model
model %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_adam(),   # use other optimizers like optimizer_sgd() if needed
metrics = c('accuracy')
)
history <- model %>% fit(
x = X_train,
y = y_train,
epochs = 20,             # Number of training iterations
batch_size = 32           # Number of samples per gradient update
)
accuracy <- model %>% evaluate(X, y)[[2]]
accuracy <- model %>% evaluate(X, y)[[1]]
subset_undrafted <- drafted_0 %>% sample_n(4000, replace = FALSE)
mini_undrafted <- subset_undrafted %>% sample_frac(0.8)
small_train <- bind_rows(drafted_1, mini_undrafted)
small_test <- subset_undrafted %>% anti_join(small_train)
small_train <- select(training_data, -pick, -player_name, -team)
small_test <- select(testing_data, -pick, -player_name, -team)
preprocess_data <- function(data) {
data[] <- lapply(data, function(x) {
if(is.numeric(x)) {
x[is.infinite(x) | is.nan(x)] <- 0
x[is.na(x)] <- mean(x, na.rm = TRUE)
return(x)
} else {
return(as.numeric(x))
}
})
return(data)
}
training_data <- preprocess_data(training_data)
testing_data <- preprocess_data(testing_data)
# Check for any remaining NAs
sum(is.na(small_train))
sum(is.na(small_test))
X_train <- small_train %>% select(`elevation (m)`, drtg, Ortg) %>% as.matrix()
y_train <- small_train$drafted
X_test <- small_test %>% select(`elevation (m)`, drtg, Ortg) %>% as.matrix()
y_test <- small_test$drafted
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = 'relu', input_shape = c(3)) %>%
layer_dense(units = 1, activation = 'sigmoid')
# Compile the model
model %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_adam(),   # use other optimizers like optimizer_sgd() if needed
metrics = c('accuracy')
)
history <- model %>% fit(
x = X_train,
y = y_train,
epochs = 20,             # Number of training iterations
batch_size = 32           # Number of samples per gradient update
)
accuracy <- model %>% evaluate(X, y)[[1]]
evaluation <- model %>% evaluate(X_test, y_test, verbose = 0)
accuracy <- evaluation[[2]]  # Access the accuracy value from the evaluation result
print(paste("Accuracy:", accuracy))
View(X_train)
X_train <- small_train %>%  as.matrix()
#having some fun with neural networks
library(rpart)
library(randomForest)
library(tidyverse)
library(rpart.plot)
library(keras)
#trouble with this data is that it can guess 0 every time and be 98% accurate because there are so few 1's in the drafted column
training_data <- read_csv("trainingData.csv")
testing_data <- read_csv("testingData.csv")
print(sum(is.na(small_train$pts)))
small_train <- select(training_data, -pick, -player_name, -team, -year)
small_test <- select(testing_data, -pick, -player_name, -team, -year, )
preprocess_data <- function(data) {
data[] <- lapply(data, function(x) {
if(is.numeric(x)) {
x[is.infinite(x) | is.nan(x)] <- 0
x[is.na(x)] <- mean(x, na.rm = TRUE)
return(x)
} else {
return(as.numeric(x))
}
})
return(data)
}
training_data <- preprocess_data(training_data)
testing_data <- preprocess_data(testing_data)
# Check for any remaining NAs
sum(is.na(small_train))
sum(is.na(small_test))
#neural network
X_train <- small_train %>% select(-pid, -drafted) %>%  as.matrix()
y_train <- small_train$drafted
X_test <- small_test %>% select(-pid, -drafted) %>% as.matrix()
y_test <- small_test$drafted
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = 'relu', input_shape = c(3)) %>%
layer_dense(units = 1, activation = 'sigmoid')
# Compile the model
model %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_adam(),   # use other optimizers like optimizer_sgd() if needed
metrics = c('accuracy')
)
# Train the model
history <- model %>% fit(
x = X_train,
y = y_train,
epochs = 20,             # Number of training iterations
batch_size = 32           # Number of samples per gradient update
)
View(X_train)
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = 'relu', input_shape = c(9)) %>%
layer_dense(units = 1, activation = 'sigmoid')
# Compile the model
model %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_adam(),   # use other optimizers like optimizer_sgd() if needed
metrics = c('accuracy')
)
history <- model %>% fit(
x = X_train,
y = y_train,
epochs = 20,             # Number of training iterations
batch_size = 32           # Number of samples per gradient update
)
reticulate::py_last_error()
small_train <- select(training_data, -pick, -player_name, -team, -year)
small_test <- select(testing_data, -pick, -player_name, -team, -year, )
preprocess_data <- function(data) {
data[] <- lapply(data, function(x) {
if(is.numeric(x)) {
x[is.infinite(x) | is.nan(x)] <- 0
x[is.na(x)] <- mean(x, na.rm = TRUE)
return(x)
} else {
return(as.numeric(x))
}
})
return(data)
}
training_data <- preprocess_data(training_data)
testing_data <- preprocess_data(testing_data)
sum(is.na(small_train))
sum(is.na(small_test))
print(is.na(small_train))
print(sum(final_data$drafted))
subset_undrafted <- drafted_0 %>% sample_n(1435, replace = FALSE)
mini_undrafted <- subset_undrafted %>% sample_frac(0.8)
small_train <- bind_rows(drafted_1, mini_undrafted)
small_test <- subset_undrafted %>% anti_join(small_train)
# Combine sampled data for training dataset
training_data <- bind_rows(drafted_1, drafted_0)
# Create testing dataset by removing rows used in training dataset
testing_data <- final_data %>% anti_join(training_data)
write_csv(training_data, "trainingData.csv")
write_csv(testing_data, "testingData.csv")
write_csv(small_test, "small_test.csv")
write_csv(small_train, "small_train.csv")
small_train <- select(training_data, -pick, -player_name, -team, -year)
small_test <- select(testing_data, -pick, -player_name, -team, -year, )
preprocess_data <- function(data) {
data[] <- lapply(data, function(x) {
if(is.numeric(x)) {
x[is.infinite(x) | is.nan(x)] <- 0
x[is.na(x)] <- mean(x, na.rm = TRUE)
return(x)
} else {
return(as.numeric(x))
}
})
return(data)
}
training_data <- preprocess_data(small_train)
testing_data <- preprocess_data(small_test)
# Check for any remaining NAs
sum(is.na(small_train))
sum(is.na(small_test))
print(is.na(small_train))
sum(is.na(small_train))
sum(is.na(small_test))
# Check for any remaining NAs
sum(is.na(training_data))
sum(is.na(testing_data))
preprocess_data <- function(data) {
data[] <- lapply(data, function(x) {
if(is.numeric(x)) {
x[is.infinite(x) | is.nan(x)] <- 0
x[is.na(x)] <- mean(x, na.rm = TRUE)
return(x)
} else {
return(as.numeric(x))
}
})
return(data)
}
training_data <- preprocess_data(small_train)
testing_data <- preprocess_data(small_test)
# Check for any remaining NAs
sum(is.na(training_data))
sum(is.na(testing_data))
small_train <- bind_rows(drafted_1, mini_undrafted)
small_test <- subset_undrafted %>% anti_join(small_train)
small_train <- select(small_train, -pick, -player_name, -team, -year)
small_test <- select(small_test, -pick, -player_name, -team, -year, )
training_data <- preprocess_data(small_train)
testing_data <- preprocess_data(small_test)
sum(is.na(training_data))
sum(is.na(testing_data))
print(is.na(training_data))
X_train <- training_data %>% select(-pid, -drafted) %>%  as.matrix()
y_train <- training_data$drafted
X_test <- testing_data %>% select(-pid, -drafted) %>% as.matrix()
y_test <- testing_data$drafted
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = 'relu', input_shape = c(9)) %>%
layer_dense(units = 1, activation = 'sigmoid')
# Compile the model
model %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_adam(),   # use other optimizers like optimizer_sgd() if needed
metrics = c('accuracy')
)
history <- model %>% fit(
x = X_train,
y = y_train,
epochs = 20,             # Number of training iterations
batch_size = 32           # Number of samples per gradient update
)
evaluation <- model %>% evaluate(X_test, y_test, verbose = 0)
accuracy <- evaluation[[2]]  # Access the accuracy value from the evaluation result
print(paste("Accuracy:", accuracy))
View(small_train)
sum(is.na(training_data))
sum(is.na(testing_data))
na.omit(training_data)
sum(is.na(training_data))
training_data <- na.omit(training_data)
training_data <- Inf.omit(training_data)
training_data <- na.omit(training_data)
sum(is.na(training_data))
X_train <- training_data %>% select(-pid, -drafted) %>%  as.matrix()
y_train <- training_data$drafted
X_test <- testing_data %>% select(-pid, -drafted) %>% as.matrix()
y_test <- testing_data$drafted
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = 'relu', input_shape = c(9)) %>%
layer_dense(units = 1, activation = 'sigmoid')
# Compile the model
model %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_adam(),   # use other optimizers like optimizer_sgd() if needed
metrics = c('accuracy')
)
# Train the model
history <- model %>% fit(
x = X_train,
y = y_train,
epochs = 20,             # Number of training iterations
batch_size = 32           # Number of samples per gradient update
)
evaluation <- model %>% evaluate(X_test, y_test, verbose = 0)
accuracy <- evaluation[[2]]  # Access the accuracy value from the evaluation result
print(paste("Accuracy:", accuracy))
knitr::opts_chunk$set(echo = TRUE)
install.packages("tidyverse")
library(tidyr)
library(dplyr)
library(readr)
jeth_data <- read_csv("jethro_data.csv")
filtered_games <- jeth_data %>% select(PLAYER_ID, gameDate, MIN, FGM, FGA, FG3M, FG3A, FTM, FTA, REB, AST, STL, BLK, TO, PTS)
trial <- filtered_games
hours_minutes <- strsplit(trial$MIN, ":")
hours <- sapply(hours_minutes, function(x) as.numeric(x[1]))
minutes <- sapply(hours_minutes, function(x) as.numeric(x[2]))
rounded <- ceiling(minutes)
# Converting hours and minutes to decimal format
decimal_hours <- hours + minutes / 60
# Updating the MIN column in df with decimal format
trial$MIN <- decimal_hours
calc_scores <- function(this_year, data){
player_id_list <- data %>% select(PLAYER_ID)
uPlayer_id_list <- unique(player_id_list)
transformed_uPlayer_id_list <- pivot_wider(uPlayer_id_list, names_from = PLAYER_ID, values_from = PLAYER_ID)
#create a new tibble and in a for loop, add player ID and their average in important categories to a row in the tibble
player_averages <- tibble(
PLAYER_ID = numeric(0),
A_FGM = numeric(0),
A_FG3M = numeric(0),
A_FTM = numeric(0),
A_REB = numeric(0),
A_AST = numeric(0),
A_STL = numeric(0),
A_BLK = numeric(0),
A_PTS = numeric(0)
)
#this gives average values for each category from 2003 games and stores them in a tibble.
means <- data %>%
summarise(
meanfgm = mean(FGM, na.rm = T),
meanfg3m = mean(FG3M, na.rm = T),
meanftm = mean(FTM, na.rm = T),
meanreb = mean(REB, na.rm = T),
meanast = mean(AST, na.rm = T),
meanstl = mean(STL, na.rm = T),
meanblk = mean(BLK, na.rm = T),
meanpts = mean(PTS, na.rm = T)
)
player_scores <- tibble(player_id = integer(), player_score = numeric(), year = numeric())
#will need to turn this into a function at some point, will make doing this for each year really easy
for (pnum in transformed_uPlayer_id_list){
this_player_games <- filter(data, PLAYER_ID == pnum)
avg_fgm <- mean(this_player_games$FGM, na.rm = T)
avg_fg3m <- mean(this_player_games$FG3M, na.rm = T)
avg_ftm <- mean(this_player_games$FTM, na.rm = T)
avg_reb <- mean(this_player_games$REB, na.rm = T)
avg_ast <- mean(this_player_games$AST, na.rm = T)
avg_stl <- mean(this_player_games$STL, na.rm = T)
avg_blk <- mean(this_player_games$BLK, na.rm = T)
avg_pts <- mean(this_player_games$PTS, na.rm = T)
player_averages <- player_averages %>%
add_row(
PLAYER_ID = pnum,
A_FGM = avg_fgm,
A_FG3M = avg_fg3m,
A_FTM = avg_ftm,
A_REB = avg_reb,
A_AST = avg_ast,
A_STL = avg_stl,
A_BLK = avg_blk,
A_PTS = avg_pts)
#at this point we have a tibble "player_averages" that has average value in each column for each player in 2003.We now want to find "player rating" which will be calculated by percent better or worse than average in each category.
summary(player_averages)
#leave out score for PTS because it is reflected in the other 3 columns, FGM, FG3M, FTM
#weights can be adjusted if we want to make some things more valuable
FGscore <- 10 * (avg_fgm / means$meanfgm - 1)
FG3score <- 10 * (avg_fg3m / means$meanfg3m- 1)
FTscore <- 10 * (avg_ftm / means$meanftm - 1)
REBscore <- 10 * (avg_reb / means$meanreb - 1)
ASTscore <- 10 * (avg_ast / means$meanast- 1)
STLscore <- 10 * (avg_stl / means$meanstl - 1)
BLKscore <- 10 * (avg_blk / means$meanblk - 1)
scores <- c(FGscore, FG3score, FTscore, REBscore, ASTscore, STLscore, BLKscore)
scores <- coalesce(scores, 0)
#print(scores)
total_score <- sum(scores)
#if(total_score == 0){
# print(pnum)
#}
#add player ID and their total score to player_scores tibble
player_scores <- player_scores %>%
add_row(
player_id = pnum,
player_score = total_score,
year = this_year)
}
#group and bin players into 6 equal size groups, 1 being lowest scores and 6 being highest.
group_size <- ceiling(nrow(player_scores) / 6)
player_scores <- player_scores %>%
#mutate(player_rank = cut(player_score, breaks = quantile(player_score, probs = seq(0,1,length.out = 6)), labels = FALSE))
mutate(player_rank = ntile(player_score, 6))
return(player_scores)
}
#run a for loop to loop through each year in the dataset and add player scores for each year
games_from_2003 <- filter(filtered_games, (gameDate > '2003-07-01' & gameDate < '2004-06-30'))
games_from_2004 <- filter(filtered_games, (gameDate > '2004-07-01' & gameDate < '2005-06-30'))
games_from_2005 <- filter(filtered_games, (gameDate > '2005-07-01' & gameDate < '2006-06-30'))
games_from_2006 <- filter(filtered_games, (gameDate > '2006-07-01' & gameDate < '2007-06-30'))
games_from_2007 <- filter(filtered_games, (gameDate > '2007-07-01' & gameDate < '2008-06-30'))
games_from_2008 <- filter(filtered_games, (gameDate > '2008-07-01' & gameDate < '2009-06-30'))
games_from_2009 <- filter(filtered_games, (gameDate > '2009-07-01' & gameDate < '2010-06-30'))
games_from_2010 <- filter(filtered_games, (gameDate > '2010-07-01' & gameDate < '2011-06-30')) #i think this last condition needs to change
games_from_2011 <- filter(filtered_games, (gameDate > '2011-07-01' & gameDate < '2012-06-30'))
games_from_2012 <- filter(filtered_games, (gameDate > '2012-07-01' & gameDate < '2013-06-30'))
games_from_2013 <- filter(filtered_games, (gameDate > '2013-07-01' & gameDate < '2014-06-30'))
games_from_2014 <- filter(filtered_games, (gameDate > '2014-07-01' & gameDate < '2015-06-30'))
games_from_2015 <- filter(filtered_games, (gameDate > '2015-07-01' & gameDate < '2016-06-30'))
games_from_2016 <- filter(filtered_games, (gameDate > '2016-07-01' & gameDate < '2017-06-30'))
games_from_2017 <- filter(filtered_games, (gameDate > '2017-07-01' & gameDate < '2018-06-30'))
games_from_2018 <- filter(filtered_games, (gameDate > '2018-07-01' & gameDate < '2019-06-30'))
games_from_2019 <- filter(filtered_games, (gameDate > '2019-07-01' & gameDate < '2020-06-30'))
games_from_2020 <- filter(filtered_games, (gameDate > '2020-07-01' & gameDate < '2021-06-30'))
games_yearly_list <- list(games_from_2003,games_from_2004,games_from_2005,games_from_2006,games_from_2007,games_from_2008,games_from_2009,games_from_2010,games_from_2011,games_from_2012,games_from_2013,games_from_2014,games_from_2015,games_from_2016,games_from_2017,games_from_2018,games_from_2019,games_from_2020)
all_player_scores <- tibble(player_id = integer(), player_score = numeric(), year = numeric())
for(i in 1:length(games_yearly_list)){
year <- 2002+i
this_data <- games_yearly_list[[i]]
this_year_scores <- calc_scores(this_year = year, data = this_data)
all_player_scores <- bind_rows(all_player_scores, this_year_scores)
}
no_zeroes <- all_player_scores %>% filter(player_score>0 | player_score<0)
hist(x=no_zeroes$player_score)
summary(no_zeroes)
write_csv(all_player_scores, "player_scores.csv")
wins_losses_per <- read_csv("wins_losses.csv")
wins_losses_per$TEAM_ID <- NA
wins_losses_per$WIN <- NA
wins_losses_per$LOSS <- NA
wins_losses_per$PCT <- NA
#the dumbest way to link the tables
for(i in 1:nrow(wins_losses_per)){
this <- wins_losses_per[[i, "Team"]]
if(this == "Utah Jazz"){team_id <- 1610612762}
if(this == "Phoenix Suns"){team_id <- 1610612756}
if(this == "Philadelphia 76ers"){team_id <- 1610612755}
if(this == "Brooklyn Nets" | this == "New Jersey Nets"){team_id <- 1610612751}
if(this == "Denver Nuggets"){team_id <- 1610612743}
if(this == "Los Angeles Clippers"){team_id <- 1610612746}
if(this == "Milwaukee Bucks"){team_id <- 1610612749}
if(this == "Dallas Mavericks"){team_id <- 1610612742}
if(this == "Los Angeles Lakers"){team_id <- 1610612742}
if(this == "Portland Trail Blazers"){team_id <- 1610612757}
if(this == "Atlanta Hawks"){team_id <- 1610612737}
if(this == "New York Knicks"){team_id <- 1610612752}
if(this == "Miami Heat"){team_id <- 1610612748}
if(this == "Golden State Warriors"){team_id <- 1610612744}
if(this == "Memphis Grizzlies"){team_id <- 1610612763}
if(this == "Boston Celtics"){team_id <- 1610612738}
if(this == "Washington Wizards"){team_id <- 1610612764}
if(this == "Indiana Pacers"){team_id <- 1610612754}
if(this == "Charlotte Hornets" | this == "New Orleans/Oklahoma City Hornets" | this == "Charlotte Bobcats"){team_id <- 1610612766}
if(this == "San Antonio Spurs"){team_id <- 1610612759}
if(this == "Chicago Bulls"){team_id <- 1610612741}
if(this == "New Orleans Pelicans"){team_id <- 1610612740}
if(this == "Sacramento Kings"){team_id <- 1610612758}
if(this == "Toronto Raptors"){team_id <- 1610612761}
if(this == "Minnesota Timberwolves"){team_id <- 1610612750}
if(this == "Cleveland Cavaliers"){team_id <- 1610612739}
if(this == "Oklahoma City Thunder" | this == "Seattle Supersonics"){team_id <- 1610612760}
if(this == "Orlando Magic"){team_id <- 1610612753}
if(this == "Detroit Pistons"){team_id <- 1610612765}
wins_losses_per[[i, "TEAM_ID"]] <- team_id
#also want to split "Record" into Wins and Losses
parts <- unlist(strsplit(wins_losses_per$Record[i], "-"))
if(length(parts[1])>2){print(parts)}
wins_losses_per$WIN[i] <- as.integer(parts[1])
wins_losses_per$LOSS[i] <- as.integer(parts[2])
wins_losses_per$PCT[i] <- (as.integer(parts[1])/(as.integer(parts[1])+as.integer(parts[2])))
}
summary(wins_losses_per)
all_player_scores <- all_player_scores %>% rename("SEASON"=year)
all_player_scores <- all_player_scores %>% rename("PLAYER_ID"=player_id)
plot(wins_losses_per$PCT)
#combine jethro_victor_data and this new data into one final dataset
jeth_vic <- read_csv("jethro_victor_data.csv")
final_dataset <- merge(jeth_vic, wins_losses_per, by = c("TEAM_ID", "SEASON"))
final_dataset <- merge(final_dataset, all_player_scores, by = c("PLAYER_ID", "SEASON"))
final_dataset <- final_dataset %>% select(-Team, -Record)
write_csv(final_dataset, "final_dataset.csv")
#final_dataset
View(final_dataset)
plot(final_dataset$player_rank)
plot(final_dataset$player_rank)
